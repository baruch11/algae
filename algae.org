#+TITLE: Algae
#+PROPERTY: header-args:python :session /Users/charlesprat/Library/Jupyter/runtime/kernel-a3a9c270-209d-4474-b519-4d557868c63e.json
#+PROPERTY: header-args:python+ :pandoc t
#+PROPERTY: header-args:python+ :dir .
#+PROPERTY: header-args:python+ :tangle yes
#+PROPERTY: header-args:python+ :exports both

* Chargement des données

/Description des données :/

#+begin_src python

  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  np.random.seed(42)

  df = pd.read_csv("./algae-1.csv", sep=";").drop(columns=["Unnamed: 0"])

  def str2float(string):
	return float(string.replace(",","."))
  df.wind = df.wind.apply(str2float)
  df.discharge = df.discharge.apply(str2float)
  df.rainfall = df.rainfall.apply(str2float)
  df.temperature = df.temperature.apply(str2float)

  df.describe().style.format(precision=1)

#+end_src

#+RESULTS:
:RESULTS:
|       | wind   | rainfall | discharge | temperature | nb.tides | risk.label |
|-------+--------+----------+-----------+-------------+----------+------------|
| count | 4789.0 | 4789.0   | 4789.0    | 4789.0      | 4789.0   | 4789.0     |
| mean  | 82.4   | 15.6     | 6554.6    | 12.6        | 4.5      | 0.3        |
| std   | 20.7   | 36.0     | 10177.0   | 6.4         | 2.2      | 0.4        |
| min   | -19.1  | 2.2      | 1863.1    | 0.1         | 0.0      | 0.0        |
| 25%   | 68.4   | 2.9      | 3324.5    | 7.9         | 3.0      | 0.0        |
| 50%   | 83.6   | 3.1      | 3667.2    | 12.0        | 4.0      | 0.0        |
| 75%   | 97.5   | 13.3     | 4388.8    | 16.7        | 6.0      | 1.0        |
| max   | 140.6  | 709.3    | 213128.2  | 41.8        | 14.0     | 1.0        |
:END:


* Exploration des données

#+begin_src python :exports none
    from pandas_profiling import ProfileReport
    profile = ProfileReport(df, title="Report")
    profile.to_file("algae.html")
#+end_src

#+RESULTS[685e1b0cefa59687ed6e9c09837d4022eb8c60c0]:
:RESULTS:
: Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]
: Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]
: Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]
: Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]
:END:




** Distribution des features en fonction de la cible 'risk.label'

#+begin_src python :exports both

  def plot_feat_vs_label(ax, feature):

      bins = np.linspace(np.quantile(df[feature],.01),
			 np.quantile(df[feature],.99),
			 50)
      kwargs = {"bins": bins, "alpha": .5, "density": True}
      ax.hist(df.loc[df["risk.label"] == 1, feature], label="risk label = 1", **kwargs)
      ax.hist(df.loc[df["risk.label"] == 0, feature], label="risk label = 0", **kwargs)
      ax.set_title(feature)
      ax.legend()

  f, vax = plt.subplots(2, 3, figsize=(10, 7))

  fax = vax.flat

  for feature in df.columns[:-1]:
      ax = next(fax)
      plot_feat_vs_label(ax, feature)


#+end_src

#+RESULTS[7a61505daa6eeafdb70cc3ffb80c725a08230a9d]:
[[file:./.ob-jupyter/dcaa095aaede5b5cbc3b7568f8ebee25fc878026.png]]

Manisfestement le rainfall est parfaitement discriminant pour notre prédiction sur 'risk.label'.
Vérification en prenant un seuil à 6 :

#+begin_src python :exports both

  df.groupby("risk.label").agg(
      a=("rainfall",lambda x: sum(x<6)),
      b=("rainfall",lambda x: sum(x>=6))
  ).rename(columns={'a': "rainfall < 6", "b": "rainfall >= 6"})


#+end_src

#+RESULTS[be872c665054347f8d9c735c22229186911ba37c]:
:RESULTS:
|            | rainfall < 6 | rainfall >= 6 |
|------------+--------------+---------------|
| risk.label |              |               |
| 0          | 3467         | 0             |
| 1          | 0            | 1322          |
:END:


Ensuite dans l'ordre des features qui paraissent être de la discriminante à la moins, on trouve : discharge, wind, nb.tides et enfin température qui semble très peu influente sur notre prédiction.

On poursuit l'étude en considérant que nous n'avons pas accès au rainfall.


** Corrélation des features entre elles

#+begin_src python :exports both
  import seaborn as sns
    
  corr = df.corr(method = 'kendall')

  sns.heatmap(corr, annot = True)

  plt.show()
#+end_src

#+RESULTS[67aaafa1f544773b02f59f68b91a9abe7d1cdd8a]:
[[file:./.ob-jupyter/103229f3a8de631c6f433428a7bdb88b097dea0b.png]]



On confirme bien la faible influence de la température. Si on se passe de rainfall, on voit que discharge va devenir la feature la plus importante. Elle est d'ailleurs aussi très corrélée à rainfall.


** Autocorrelation de chaque feature


#+begin_src python :exports both

  import matplotlib.pyplot as plt
  def my_autocorr_plot(feat, ax):
      xn = df[feat]-df[feat].mean()
      xn = xn/np.sqrt((xn**2).mean())
      ax.plot(np.roll(np.correlate(xn,xn,'full'),len(xn))[:10] / len(xn))
      ax.set_title(feat)

  f, (ax, ax1, ax2) = plt.subplots(1,3, figsize=(5,3))
  my_autocorr_plot("wind", ax)
  my_autocorr_plot("temperature", ax1)
  my_autocorr_plot("discharge", ax2)

  f.suptitle("Autocorrelations")
  plt.tight_layout()
  plt.show()
#+end_src

#+RESULTS[d568bfd3b23af2b464f56c1f9adf096eff481c73]:
[[file:./.ob-jupyter/ccca2e3916ac759b8a835c4bc509e9a8d51dcaf5.png]]



* Performances des modèles de base

** Seuil sur le discharge


#+begin_src python :exports both
  thresh = 4000
  df.groupby("risk.label").agg(
      a=("discharge",lambda x: sum(x <thresh )),
      b=("discharge",lambda x: sum(x >= thresh))
  ).rename(columns={'a': f"discharge < {thresh}", "b": f"discharge >= {thresh}"})


#+end_src

#+RESULTS[6a100c622bf40cad4f2a0ba683c88e48a5014be7]:
:RESULTS:
|            | discharge < 4000 | discharge >= 4000 |
|------------+------------------+-------------------|
| risk.label |                  |                   |
| 0          | 3105             | 362               |
| 1          | 133              | 1189              |
:END:


#+begin_src python :exports both
from sklearn.metrics import f1_score, recall_score, precision_score
y_pred = df.discharge > thresh
y_true = df["risk.label"]

print(f"precision: {precision_score(y_true, y_pred):.2f}")
print(f"recall: {recall_score(y_true, y_pred):.2f}")
print(f"f1: {f1_score(y_true, y_pred):.2f}")

#+end_src

#+RESULTS[15dbcc173f2a7db747fd782eacd8044be2babd91]:
: precision: 0.77
: recall: 0.90
: f1: 0.83




** Quelques modèles basiques

#+begin_src python :exports none
  from sklearn.model_selection import train_test_split

  features = df.drop(columns=["rainfall", "risk.label"])
  target = df["risk.label"]
  X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.15)

  target.mean(), y_test.mean()

#+end_src

#+RESULTS[b32c7ff6276fa262b06dd814ba99df08fbd9f216]:
| 0.2760492795990812 | 0.29763560500695413 |


#+begin_src python :exports both
  from sklearn.linear_model import LogisticRegression
  from sklearn.naive_bayes import GaussianNB

  models = {"naive bayes": GaussianNB(),
	    "logistic regression": LogisticRegression()}
  # fit models
  for name, model in models.items():
      model.fit(X_train, y_train)

  def my_scores(model):
      return {
	  "precision": precision_score(y_test, model.predict(X_test)),
	  "recall": recall_score(y_test, model.predict(X_test)),
	  "f1": f1_score(y_test, model.predict(X_test)), }

  scores = pd.DataFrame.from_dict(
      {name: my_scores(model)
	    for name, model in models.items()}, orient='index'
  )


  display(scores.style.format(precision=2))

#+end_src

#+RESULTS[93529832049286af480039859ae96aa9b43e3658]:
:RESULTS:
|                     | precision | recall | f1   |
|---------------------+-----------+--------+------|
| naive bayes         | 0.99      | 0.86   | 0.92 |
| logistic regression | 0.98      | 0.86   | 0.92 |
:END:


* Estimation bayesienne


Si on connait la vraisemmblance /f/ des données /D/ sachant le risk.label, on peut prendre une décision basée sur la formule de Bayes. En effet on a:

\begin{equation}
{\frac{P(risk = 1 | D, \theta)}{P(risk = 0 | D, \theta)}}\approx{\frac{f(D|risk = 1, \hat{\theta}).P(risk = 1)}{f(D|risk=0, \hat{\theta}).P(risk = 0)}}
\end{equation}



** Modélisation

*** indépendance température et risk.label

#+begin_src python

  from scipy import stats

  stats.spearmanr(df.temperature, df["risk.label"])
  #stats.spearmanr(df.discharge, df["risk.label"])
  #stats.spearmanr(np.random.randn(1000), np.random.randn(1000))

#+end_src

#+RESULTS[ab82d473d3c30984e26a0419bba71ca4b5d0e391]:
: SpearmanrResult(correlation=0.06062450480098488, pvalue=2.6911644391237046e-05)


*** recherche des types de distribution

**** QQ-plots 



#+begin_src python :exports both

  from scipy import stats
  f, vax = plt.subplots(2,4, figsize=(10,5), sharex=True)
  fax = vax.flat
  iax = iter(fax)

  label0 = df["risk.label"]==0

  def dist_vs_target(feature, ax1,ax2, dist1=stats.norm, dist2=stats.norm):
	stats.probplot(df.loc[label0,feature], plot=ax1, dist=dist1)
	ax1.set_title(f"{feature} | risk.label = 0\ndist: {dist1.name}")
	ax1.set_xlabel("")
	stats.probplot(df.loc[~label0,feature], plot=ax2, dist=dist2)
	ax2.set_title(f"{feature} | risk.label = 0\ndist: {dist2.name}")
	ax2.set_xlabel("")

  dist_vs_target("wind", next(iax), next(iax))
  dist_vs_target("discharge", next(iax), next(iax), dist2=stats.expon)
  dist_vs_target("temperature", next(iax), next(iax))
  #dist_vs_target("nb.tides", next(iax), next(iax),dist1=stats.binom)

  f.tight_layout(pad=1.0)
  f.suptitle("QQ-plots")
  plt.tight_layout()
  plt.show()

 #+end_src

 #+RESULTS[54a45ad21b729a47d942baf7f53a7ba8b233e47f]:
 [[file:./.ob-jupyter/547ef128e6396d7848967395925677f77d9a7314.png]]

 
**** fit du modèle complet

***** Fit de la loi de nb.tides

#+begin_src python

  def fit_poisson(samples):

      vmus = np.arange(1,30,0.1)
      vlog_llh = np.array([np.sum(stats.poisson.logpmf(samples,vmu))
			   for vmu in vmus])

      return vmus[np.argmax(vlog_llh)]

  mu_risk0 = fit_poisson(df.loc[label0, "nb.tides"])
  mu_risk1 = fit_poisson(df.loc[~label0, "nb.tides"])
  print(f"mu for riskO: {mu_risk0}"
	f"mu for risk1: {mu_risk1}")

#+end_src

#+RESULTS:
: mu for riskO: 4.000000000000003mu for risk1: 5.600000000000004

***** Fit des autres features


#+begin_src python
from libalgae import AlgaeClassifier, Distrib

distrib_dict = {
      "wind": (Distrib(stats.norm), Distrib(stats.norm)),
      #"temperature": (Distrib(stats.norm), Distrib(stats.norm)),
      "discharge": (Distrib(stats.norm), Distrib(stats.weibull_min)),
      "nb.tides": (Distrib(stats.poisson, [mu_risk0]),
                   Distrib(stats.poisson, [mu_risk1]))
  
  
  }
clf = AlgaeClassifier(distrib_dict)
clf.fit(X_train, y_train, en_prior=True)  

#+end_src

#+RESULTS:

***** Visualisations de la fitness


#+begin_src python
  
  feature_name = "discharge"
  risk0 = df["risk.label"] == 0

  f, vax = plt.subplots(1,2, figsize=(5,3))

  iax = iter(vax)

  ax = next(iax)
  ax.hist(df.loc[risk0, feature_name], density=True, alpha=.3)
  clf.plot_fitted_distrib(ax, feature_name, 0)
  ax.set_title("risk = 0")
  ax.legend()

  ax = next(iax)
  ax.hist(df.loc[~risk0, feature_name], density=True, alpha=.3, bins=50)
  clf.plot_fitted_distrib(ax, feature_name, 1)
  ax.set_title("risk = 1")
  ax.legend()

  f.suptitle(feature_name)
  plt.tight_layout()
  plt.show()

#+end_src

#+RESULTS[1aad9c1d34ee175526a487f707e51f0efe2ab789]:
[[file:./.ob-jupyter/b9519682f20ab69f4a11735761be4d4d92aa944c.png]]


#+begin_src python
  from scipy import stats
  f, (ax, ax1) = plt.subplots(1,2,figsize=(6,3))
  ax.hist(df.loc[label0, "nb.tides"], bins=np.arange(0,20), align = 'left', rwidth=.5, density=True, alpha=.3)
  vx = np.arange(0,20)
  ax.plot(vx, stats.poisson.pmf(vx, mu_risk0), '+')

  ax1.hist(df.loc[~label0, "nb.tides"], bins=np.arange(0,20), align = 'left', rwidth=.5, density=True, alpha=.3)
  ax1.plot(vx, stats.poisson.pmf(vx, mu_risk1), '+')

  plt.show()

#+end_src

#+RESULTS:
[[file:./.ob-jupyter/56abef712e4322c305b9bd4f093eb5e7890e4476.png]]


** Performance finale

#+begin_src python

  p1 = y_train.mean()
  results = pd.DataFrame(
	data=[clf.get_scores(X_train, y_train),
	      clf.get_scores(X_test, y_test)],
	index=["train", "test"],

    )

  results.style.format(precision=3)

#+end_src

#+RESULTS:
:RESULTS:
|       | accuracy | f1    | precision | recall |
|-------+----------+-------+-----------+--------|
| train | 0.960    | 0.923 | 0.966     | 0.884  |
| test  | 0.955    | 0.920 | 0.989     | 0.860  |
:END:

